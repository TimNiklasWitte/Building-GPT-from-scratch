{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf3bf23",
   "metadata": {},
   "source": [
    "## Util Definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94ff6b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "def extract_test_and_vald_set(text, test_size = 0.1, validation_size = 0.1):\n",
    "    total_length = len(text)\n",
    "    test_length = int(total_length * test_size)\n",
    "    val_length = int(total_length * validation_size)\n",
    "\n",
    "    test_start_index = random.randint(0, total_length - test_length)\n",
    "    test_end_index = test_start_index + test_length\n",
    "    test_text = text[test_start_index:test_end_index]\n",
    "\n",
    "    total_length_without_test = total_length - test_length\n",
    "    val_start_index = random.randint(0, total_length_without_test - val_length)\n",
    "    val_end_index = val_start_index + val_length\n",
    "    validation_text = text[val_start_index:val_end_index]   \n",
    "\n",
    "    train_text = text[:val_start_index] + text[val_end_index:]  \n",
    "\n",
    "    return train_text, test_text, validation_text\n",
    "\n",
    "def encode_vocab_to_index(vocabulary):\n",
    "    stoi = {token:i for i, token in enumerate(vocabulary) }\n",
    "    def encode(token_list):\n",
    "        return [stoi[token] for token in token_list]\n",
    "    return encode\n",
    "\n",
    "def decode_index_to_vocab(vocabulary):\n",
    "    itos = {i: token for i, token in enumerate(vocabulary)}\n",
    "    def decode(index_list):\n",
    "        return [itos[i] for i in index_list]\n",
    "    return decode\n",
    "\n",
    "def save_vocabulary_or_merges(vocabulary, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for token in vocabulary:\n",
    "            file.write(f\"{token}\\n\")\n",
    "\n",
    "def save_document(text, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "def open_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a772a29e",
   "metadata": {},
   "source": [
    "### Hypterparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84c8da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ad668",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d63cb8a",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32e409a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class BPE():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def learner(corpus, merge_count=150):\n",
    "        corpus = corpus.lower()\n",
    "        words = [list(word) + ['_'] for word in corpus.split()]\n",
    "\n",
    "        merges = []\n",
    "\n",
    "        for m in range(merge_count):\n",
    "            vocab = defaultdict(int)\n",
    "            for word in words:\n",
    "                for i in range(len(word) - 1):\n",
    "                    pair = (word[i], word[i+1])\n",
    "                    vocab[pair] += 1\n",
    "\n",
    "            most_frequent = max(vocab, key=vocab.get)\n",
    "            merges.append(most_frequent)\n",
    "\n",
    "            new_token = ''.join(most_frequent)\n",
    "            new_words = []\n",
    "            for word in words:\n",
    "                new_word = []\n",
    "                i = 0\n",
    "                while i < len(word):\n",
    "                    # Merge durchführen\n",
    "                    if i < len(word) - 1 and (word[i], word[i+1]) == most_frequent:\n",
    "                        new_word.append(new_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_word.append(word[i])\n",
    "                        i += 1\n",
    "                new_words.append(new_word)\n",
    "            words = new_words  # Corpus aktualisieren\n",
    "\n",
    "            print(f\"Iteration {m+1}: merged {most_frequent}\")\n",
    "\n",
    "        vocabulary = set()\n",
    "        for word in words:\n",
    "            for token in word:\n",
    "                vocabulary.add(token)\n",
    "        #vocabulary = sorted(vocabulary)\n",
    "\n",
    "\n",
    "        return merges, vocabulary\n",
    "\n",
    "    def segmenter(corpus, merges):\n",
    "        words = [list(word) + ['_'] for word in corpus.lower().split()]\n",
    "        for merge in merges:\n",
    "            new_token = ''.join(merge)\n",
    "            new_words = []\n",
    "            for word in words:\n",
    "                new_word = []\n",
    "                i = 0\n",
    "                while i < len(word):\n",
    "                    if i < len(word) - 1 and (word[i], word[i+1]) == merge:\n",
    "                        new_word.append(new_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_word.append(word[i])\n",
    "                        i += 1\n",
    "                new_words.append(new_word)\n",
    "            words = new_words\n",
    "            tokenised_corpus = [''.join(word).strip('_') for word in words]\n",
    "            flat_tokens = [token for word in words for token in word if token != '_']\n",
    "        return flat_tokens\n",
    "\n",
    "    #text_path = 'data/shakespeare.txt'\n",
    "    #corpus = open_text_file(text_path)\n",
    "\n",
    "    #train_corpus, test_corpus = extract_test_set(corpus, test_size=0.1)\n",
    "    #save_document(train_corpus, 'data/train_corpus.txt')\n",
    "    #save_document(test_corpus, 'data/test_corpus.txt')\n",
    "\n",
    "    #train_corpus = open_text_file('data/train_corpus.txt')\n",
    "\n",
    "    #new_corpus, merges, tokens = learner(train_corpus, merge_count=200)\n",
    "    #save_vocabulary(tokens, 'data/vocabulary.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c77cc",
   "metadata": {},
   "source": [
    "### Splitting Corpus in train, test, and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5040f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text, test_text, validation_text = extract_test_and_vald_set(open_text_file('data/shakespeare.txt'))\n",
    "train_corpus = open_text_file('data/Shakespeare_clean_train.txt')\n",
    "test_corpus = open_text_file('data/Shakespeare_clean_test.txt')\n",
    "validation_corpus = open_text_file('data/Shakespeare_clean_valid.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae483ccd",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9be5ca3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Text Sample: The Tragedy of Antony and Cleopatra Dramatis Personae MARK ANTONY OCTAVIUS CAESAR M. AEMILIUS LEPIDU\n",
      "Init Vocabulary:  !&',-.12689:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 67\n",
      "Iteration 1: merged ('e', '_')\n",
      "Iteration 2: merged ('t', 'h')\n",
      "Iteration 3: merged (',', '_')\n",
      "Iteration 4: merged ('t', '_')\n",
      "Iteration 5: merged ('s', '_')\n",
      "Iteration 6: merged ('d', '_')\n",
      "Iteration 7: merged ('a', 'n')\n",
      "Iteration 8: merged ('e', 'r')\n",
      "Iteration 9: merged ('o', 'u')\n",
      "Iteration 10: merged ('i', 'n')\n",
      "Iteration 11: merged ('o', '_')\n",
      "Iteration 12: merged ('y', '_')\n",
      "Iteration 13: merged ('e', 'n')\n",
      "Iteration 14: merged ('.', '_')\n",
      "Iteration 15: merged ('o', 'r')\n",
      "Iteration 16: merged ('a', 'r')\n",
      "Iteration 17: merged ('o', 'n')\n",
      "Iteration 18: merged ('l', 'l')\n",
      "Iteration 19: merged ('th', 'e_')\n",
      "Iteration 20: merged ('h', 'a')\n",
      "Iteration 21: merged ('an', 'd_')\n",
      "Iteration 22: merged ('e', 's')\n",
      "Iteration 23: merged ('i', 's_')\n",
      "Iteration 24: merged ('f', '_')\n",
      "Iteration 25: merged ('y', 'ou')\n",
      "Iteration 26: merged ('a', '_')\n",
      "Iteration 27: merged ('i', '_')\n",
      "Iteration 28: merged ('t', 'o_')\n",
      "Iteration 29: merged ('ll', '_')\n",
      "Iteration 30: merged ('n', 'o')\n",
      "Iteration 31: merged ('er', '_')\n",
      "Iteration 32: merged ('o', 'm')\n",
      "Iteration 33: merged ('w', 'i')\n",
      "Iteration 34: merged ('e', 'a')\n",
      "Iteration 35: merged (';', '_')\n",
      "Iteration 36: merged ('o', 'f_')\n",
      "Iteration 37: merged (':', '_')\n",
      "Iteration 38: merged ('e', ',_')\n",
      "Iteration 39: merged ('in', 'g')\n",
      "Iteration 40: merged ('th', '_')\n",
      "Iteration 41: merged ('t', 'i')\n",
      "Iteration 42: merged ('c', 'h')\n",
      "Iteration 43: merged ('l', 'e')\n",
      "Iteration 44: merged ('in', '_')\n",
      "Iteration 45: merged ('r', '_')\n",
      "Iteration 46: merged ('s', 't')\n",
      "Iteration 47: merged ('a', 't_')\n",
      "Iteration 48: merged ('w', 'h')\n",
      "Iteration 49: merged ('f', 'or')\n",
      "Iteration 50: merged ('v', 'e_')\n",
      "Iteration 51: merged ('m', 'y_')\n",
      "Iteration 52: merged ('l', 'i')\n",
      "Iteration 53: merged ('s', 'e')\n",
      "Iteration 54: merged ('m', 'a')\n",
      "Iteration 55: merged ('th', 'e')\n",
      "Iteration 56: merged ('o', 'w')\n",
      "Iteration 57: merged ('?', '_')\n",
      "Iteration 58: merged ('l', 'a')\n",
      "Iteration 59: merged ('c', 'a')\n",
      "Iteration 60: merged ('you', '_')\n",
      "Iteration 61: merged ('s', 'h')\n",
      "Iteration 62: merged ('!', '_')\n",
      "Iteration 63: merged ('th', 'at_')\n",
      "Iteration 64: merged ('r', 'i')\n",
      "Iteration 65: merged ('r', 'e')\n",
      "Iteration 66: merged ('ing', '_')\n",
      "Iteration 67: merged ('b', 'e')\n",
      "Iteration 68: merged ('u', 's_')\n",
      "Iteration 69: merged ('l', 'o')\n",
      "Iteration 70: merged ('r', 'a')\n",
      "Iteration 71: merged ('s', ',_')\n",
      "Iteration 72: merged ('g', 'h')\n",
      "Iteration 73: merged ('en', '_')\n",
      "Iteration 74: merged ('s', 't_')\n",
      "Iteration 75: merged ('i', 't_')\n",
      "Iteration 76: merged ('on', '_')\n",
      "Iteration 77: merged ('s', 'i')\n",
      "Iteration 78: merged ('k', '_')\n",
      "Iteration 79: merged ('no', 't_')\n",
      "Iteration 80: merged ('d', 'i')\n",
      "Iteration 81: merged ('b', 'u')\n",
      "Iteration 82: merged ('a', 's_')\n",
      "Iteration 83: merged ('t', ',_')\n",
      "Iteration 84: merged ('d', ',_')\n",
      "Iteration 85: merged ('e', 'ar')\n",
      "Iteration 86: merged ('an', '_')\n",
      "Iteration 87: merged ('m', '_')\n",
      "Iteration 88: merged ('wi', 'th_')\n",
      "Iteration 89: merged ('u', 'n')\n",
      "Iteration 90: merged ('e', 's_')\n",
      "Iteration 91: merged ('m', 'e_')\n",
      "Iteration 92: merged ('h', 'i')\n",
      "Iteration 93: merged ('th', 'ou')\n",
      "Iteration 94: merged ('u', 'r')\n",
      "Iteration 95: merged (\"'\", 's_')\n",
      "Iteration 96: merged ('l', 'd_')\n",
      "Iteration 97: merged ('e', 'd_')\n",
      "Iteration 98: merged ('ch', '_')\n",
      "Iteration 99: merged ('y', ',_')\n",
      "Iteration 100: merged ('for', '_')\n",
      "Iteration 101: merged ('s', 'a')\n",
      "Iteration 102: merged ('o', 'o')\n",
      "Iteration 103: merged ('h', 'is_')\n",
      "Iteration 104: merged ('a', 't')\n",
      "Iteration 105: merged ('c', 'om')\n",
      "Iteration 106: merged ('h', 'e_')\n",
      "Iteration 107: merged ('b', 'e_')\n",
      "Iteration 108: merged ('en', 't')\n",
      "Iteration 109: merged ('er', 'e_')\n",
      "Iteration 110: merged ('m', 'e')\n",
      "Iteration 111: merged ('bu', 't_')\n",
      "Iteration 112: merged ('s', 'e_')\n",
      "Iteration 113: merged ('you', 'r_')\n",
      "Iteration 114: merged ('t', 'a')\n",
      "Iteration 115: merged ('w', 'e')\n",
      "Iteration 116: merged ('th', 'is_')\n",
      "Iteration 117: merged ('r', 'o')\n",
      "Iteration 118: merged ('ha', 've_')\n",
      "Iteration 119: merged ('k', 'e_')\n",
      "Iteration 120: merged ('d', 'e')\n",
      "Iteration 121: merged ('w', 'ha')\n",
      "Iteration 122: merged ('c', 'e_')\n",
      "Iteration 123: merged ('f', 'a')\n",
      "Iteration 124: merged ('no', 'w')\n",
      "Iteration 125: merged ('r', 'u')\n",
      "Iteration 126: merged ('s', 'p')\n",
      "Iteration 127: merged ('s', 'ha')\n",
      "Iteration 128: merged ('r', 'om')\n",
      "Iteration 129: merged ('e', '._')\n",
      "Iteration 130: merged ('thou', '_')\n",
      "Iteration 131: merged ('or', '_')\n",
      "Iteration 132: merged ('l', '_')\n",
      "Iteration 133: merged ('ou', 'ld_')\n",
      "Iteration 134: merged ('v', 'i')\n",
      "Iteration 135: merged ('le', 't_')\n",
      "Iteration 136: merged ('m', 'i')\n",
      "Iteration 137: merged ('wi', 'll_')\n",
      "Iteration 138: merged ('l', 'e_')\n",
      "Iteration 139: merged ('an', 't')\n",
      "Iteration 140: merged ('d', 'o_')\n",
      "Iteration 141: merged ('r', 'ea')\n",
      "Iteration 142: merged ('a', 'g')\n",
      "Iteration 143: merged ('th', 'er')\n",
      "Iteration 144: merged ('wha', 't_')\n",
      "Iteration 145: merged ('f', 'i')\n",
      "Iteration 146: merged ('s', 'u')\n",
      "Iteration 147: merged ('o', ',_')\n",
      "Iteration 148: merged ('o', 'l')\n",
      "Iteration 149: merged ('q', 'u')\n",
      "Iteration 150: merged (\"'\", 'd_')\n",
      "Tokenised Training Corpus Sample: ['the_', 't', 'ra', 'g', 'e', 'd', 'y_', 'of_', 'ant', 'on', 'y_', 'and_', 'c', 'le', 'o', 'p', 'at', 'r', 'a_', 'd', 'ra', 'ma', 't', 'is_', 'p', 'er', 's', 'on', 'a', 'e_', 'm', 'ar', 'k_', 'ant', 'on', 'y_', 'o', 'c', 'ta', 'vi', 'us_', 'ca', 'es', 'ar', 'm', '._', 'a', 'e', 'mi', 'li', 'us_', 'le', 'p', 'i', 'd', 'us_', 't', 'ri', 'u', 'm', 'vi', 'r', 's', '._', 'se', 'x', 't', 'us_', 'p', 'om', 'p', 'e', 'i', 'us_', 'd', 'om', 'i', 'ti', 'us_', 'en', 'o', 'b', 'ar', 'b', 'us_', 'v', 'en', 'ti', 'di', 'us_', 'er', 'o', 's_', 's', 'c', 'ar', 'us_', 'd', 'er', 'c']\n",
      "Encoded Training Corpus Sample: [97, 51, 124, 85, 89, 39, 41, 107, 8, 7, 41, 144, 171, 130, 54, 57, 2, 135, 150, 39, 124, 172, 51, 125, 57, 82, 83, 7, 127, 35, 185, 79, 132, 8, 7, 41, 54, 171, 109, 163, 105, 76, 113, 79, 185, 168, 127, 89, 70, 161, 105, 130, 57, 189, 39, 105, 51, 84, 56, 185, 163, 135, 83, 168, 19, 165, 51, 105, 57, 17, 57, 89, 189, 105, 39, 17, 189, 141, 105, 184, 54, 123, 79, 123, 105, 38, 184, 141, 29, 105, 82, 54, 42, 83, 171, 79, 105, 39, 82, 171]\n",
      "decoded Training Ids: ['the_', 't', 'ra', 'g', 'e', 'd', 'y_', 'of_', 'ant', 'on', 'y_', 'and_', 'c', 'le', 'o', 'p', 'at', 'r', 'a_', 'd', 'ra', 'ma', 't', 'is_', 'p', 'er', 's', 'on', 'a', 'e_', 'm', 'ar', 'k_', 'ant', 'on', 'y_', 'o', 'c', 'ta', 'vi', 'us_', 'ca', 'es', 'ar', 'm', '._', 'a', 'e', 'mi', 'li', 'us_', 'le', 'p', 'i', 'd', 'us_', 't', 'ri', 'u', 'm', 'vi', 'r', 's', '._', 'se', 'x', 't', 'us_', 'p', 'om', 'p', 'e', 'i', 'us_', 'd', 'om', 'i', 'ti', 'us_', 'en', 'o', 'b', 'ar', 'b', 'us_', 'v', 'en', 'ti', 'di', 'us_', 'er', 'o', 's_', 's', 'c', 'ar', 'us_', 'd', 'er', 'c']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Take a look at the text\n",
    "print(\"Training Text Sample:\", train_corpus[:100])\n",
    "# Take a look at the current characters that occur\n",
    "chars = sorted(list(set(train_corpus)))\n",
    "print(\"Init Vocabulary:\", ''.join(chars))\n",
    "print(\"Vocabulary size:\", len(chars))\n",
    "\n",
    "# Train the BPE model\n",
    "merges, vocabulary = BPE.learner(train_corpus, merge_count=150)\n",
    "\n",
    "# Save the vocabulary and merges\n",
    "save_vocabulary_or_merges(vocabulary, 'data/vocabulary.txt')\n",
    "save_vocabulary_or_merges(merges, 'data/merges.txt')\n",
    "\n",
    "# Apply the Segmenter to the training corpus\n",
    "tokenised_train_corpus = BPE.segmenter(train_corpus, merges)\n",
    "tokenised_validation_corpus = BPE.segmenter(validation_corpus, merges)\n",
    "tokenised_test_corpus = BPE.segmenter(test_corpus, merges)\n",
    "\n",
    "print(\"Tokenised Training Corpus Sample:\", tokenised_train_corpus[:100])  # Print first 100 tokens\n",
    "\n",
    "# Encode and decode the vocabulary to a list of indices\n",
    "encode_vocab = encode_vocab_to_index(vocabulary)\n",
    "decode_vocab = decode_index_to_vocab(vocabulary)\n",
    "\n",
    "# Encode and decode the training corpus\n",
    "train_ids = encode_vocab(tokenised_train_corpus)\n",
    "test_ids = encode_vocab(tokenised_test_corpus)\n",
    "validation_ids = encode_vocab(tokenised_validation_corpus)\n",
    "\n",
    "print(\"Encoded Training Corpus Sample:\", train_ids[:100]) \n",
    "text =  decode_vocab(train_ids)\n",
    "print(\"decoded Training Ids:\", text[:100])  # Print first 100 decoded tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "033ca81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([438197]) torch.int64\n",
      "Training Corpus Encoded Sample: tensor([ 97,  51, 124,  85,  89,  39,  41, 107,   8,   7,  41, 144, 171, 130,\n",
      "         54,  57,   2, 135, 150,  39, 124, 172,  51, 125,  57,  82,  83,   7,\n",
      "        127,  35, 185,  79, 132,   8,   7,  41,  54, 171, 109, 163, 105,  76,\n",
      "        113,  79, 185, 168, 127,  89,  70, 161, 105, 130,  57, 189,  39, 105,\n",
      "         51,  84,  56, 185, 163, 135,  83, 168,  19, 165,  51, 105,  57,  17,\n",
      "         57,  89, 189, 105,  39,  17, 189, 141, 105, 184,  54, 123,  79, 123,\n",
      "        105,  38, 184, 141,  29, 105,  82,  54,  42,  83, 171,  79, 105,  39,\n",
      "         82, 171])\n"
     ]
    }
   ],
   "source": [
    "# convert the training corpus to a tensor\n",
    "train_data = torch.tensor(train_ids, dtype=torch.long)\n",
    "test_data = torch.tensor(test_ids, dtype=torch.long)\n",
    "validation_data = torch.tensor(validation_ids, dtype=torch.long)\n",
    "\n",
    "print(train_data.shape, train_data.dtype)\n",
    "print(\"Training Corpus Encoded Sample:\", train_data[:100])  # Print first 100 encoded tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3741eeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: tensor([97]), Target: 51\n",
      "Context: tensor([97, 51]), Target: 124\n",
      "Context: tensor([ 97,  51, 124]), Target: 85\n",
      "Context: tensor([ 97,  51, 124,  85]), Target: 89\n",
      "Context: tensor([ 97,  51, 124,  85,  89]), Target: 39\n",
      "Context: tensor([ 97,  51, 124,  85,  89,  39]), Target: 41\n",
      "Context: tensor([ 97,  51, 124,  85,  89,  39,  41]), Target: 107\n",
      "Context: tensor([ 97,  51, 124,  85,  89,  39,  41, 107]), Target: 8\n"
     ]
    }
   ],
   "source": [
    "input = train_data[:block_size]\n",
    "expected_output = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = input[:t+1]\n",
    "    target = expected_output[t] \n",
    "    print(f\"Context: {context}, Target: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a4de19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efca9a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix Shape: torch.Size([4, 8])\n",
      "Input Matrix: tensor([[127,  11,  97,  32,  54,  41, 120, 170],\n",
      "        [ 27,  72,  89, 131,  56,  57, 136,  97],\n",
      "        [ 47,  27,  85,  25, 158,  97, 137, 161],\n",
      "        [ 83, 171, 184,  35,  38, 168, 127, 130]])\n",
      "Output Matrix Shape: torch.Size([4, 8])\n",
      "Output Matrix: tensor([[ 11,  97,  32,  54,  41, 120, 170, 171],\n",
      "        [ 72,  89, 131,  56,  57, 136,  97,  57],\n",
      "        [ 27,  85,  25, 158,  97, 137, 161,  76],\n",
      "        [171, 184,  35,  38, 168, 127, 130, 165]])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = train_data\n",
    "    elif split == 'val':\n",
    "        data = validation_data\n",
    "    #elif split == 'test':\n",
    "    #    data = test_ids\n",
    "    else:\n",
    "        raise ValueError(\"Invalid split. Choose from 'train', 'val', or 'test'.\")\n",
    "\n",
    "    intx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    input = torch.stack([data[i:i+block_size] for i in intx])\n",
    "    expected_output = torch.stack([data[i+1:i+block_size+1] for i in intx])\n",
    "\n",
    "    return input, expected_output\n",
    "\n",
    "inputMatrix, outputMatrix = get_batch('train')\n",
    "print(\"Input Matrix Shape:\", inputMatrix.shape)  \n",
    "print(\"Input Matrix:\", inputMatrix)\n",
    "print(\"Output Matrix Shape:\", outputMatrix.shape)\n",
    "print(\"Output Matrix:\", outputMatrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d0e37fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f6edef",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a357e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cb2e0c2",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6f97dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7021f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) \n",
    "        B, T, C = logits.shape # (batch_size, block_size, vocab_size)\n",
    "        logits = logits.view(B*T, self.vocab_size)  # reshape to (batch_size * block_size, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            targets = targets.view(B*T) # reshape to (batch_size * block_size,)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # sample\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7adf50b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 0] Train Loss: 5.7060 | Val Loss: 5.7148\n",
      "[Step 300] Train Loss: 4.3800 | Val Loss: 4.4093\n",
      "[Step 600] Train Loss: 3.9480 | Val Loss: 3.9647\n",
      "[Step 900] Train Loss: 3.7993 | Val Loss: 3.8291\n",
      "[Step 1200] Train Loss: 3.7490 | Val Loss: 3.7635\n",
      "[Step 1500] Train Loss: 3.7109 | Val Loss: 3.7355\n",
      "[Step 1800] Train Loss: 3.6913 | Val Loss: 3.7275\n",
      "[Step 2100] Train Loss: 3.6804 | Val Loss: 3.7229\n",
      "[Step 2400] Train Loss: 3.6760 | Val Loss: 3.7084\n",
      "[Step 2700] Train Loss: 3.6625 | Val Loss: 3.7073\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     30\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28mprint\u001b[39m(decode_vocab(\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n",
      "Cell \u001b[1;32mIn[18], line 27\u001b[0m, in \u001b[0;36mBigramLanguageModel.generate\u001b[1;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[0;32m     26\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(idx)\n\u001b[1;32m---> 27\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     28\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     29\u001b[0m     idx_next \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# sample\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocabulary)\n",
    "\n",
    "model = BigramLanguageModel(vocab_size=vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for step in range(max_iters):\n",
    "\n",
    "    # Alle eval_interval Schritte: Trainings- und Validierungs-Loss ausgeben\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"[Step {step}] Train Loss: {losses['train']:.4f} | Val Loss: {losses['val']:.4f}\")\n",
    "\n",
    "    # Batch aus Trainingsdaten holen\n",
    "    input_batches, expected_output_batches = get_batch('train')\n",
    "\n",
    "    # Modell vorwärtslaufen lassen + Loss berechnen\n",
    "    logits, loss = model(input_batches, expected_output_batches)\n",
    "\n",
    "    # Gradienten zurücksetzen\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode_vocab(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f47a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "class N_Gram_Basic:\n",
    "\n",
    "    def __init__(self, n, vocab):\n",
    "        # n = 1 -> unigram P(wt​​)\n",
    "        # n = 2 -> bigram  P(wt​∣​wt−1​)\n",
    "        # n = 3 -> trigram P(wt​∣wt−2​,wt−1​)\n",
    "\n",
    "        assert(1 <= n)\n",
    "\n",
    "\n",
    "        self.n = n \n",
    "        self.vocab = vocab \n",
    "        \n",
    "        self.vocab_size = len(vocab)\n",
    "\n",
    "        # e.g. P(wt​∣wt−2​,wt−1​) -> order: wt−2​,wt−1, wt\n",
    "        self.cnts = {}\n",
    " \n",
    "        self.map_token_to_id = {\n",
    "            token:id for id, token in enumerate(vocab)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, corpus_tokenized):\n",
    "        \n",
    "        tokens = corpus_tokenized[0:self.n]\n",
    "        token_ids_window = [self.map_token_to_id[id] for id in tokens]\n",
    "        \n",
    "        print(f\"Train {self.n}-Gram:\")\n",
    "\n",
    "        for token in tqdm.tqdm(corpus_tokenized[self.n:], position=0, leave=True):\n",
    "       \n",
    "            try:\n",
    "                self.cnts[tuple(token_ids_window)] += 1\n",
    "\n",
    "            except KeyError:\n",
    "                self.cnts[tuple(token_ids_window)] = 1\n",
    "            \n",
    "            token_id = self.map_token_to_id[token]\n",
    "            token_ids_window.append(token_id)\n",
    "\n",
    "            token_ids_window.pop(0)\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "    def get_prob(self, token_ids_window):\n",
    "\n",
    "        #\n",
    "        # Laplace smoothing\n",
    "        #\n",
    "\n",
    "        try:\n",
    "            cnt_target = self.cnts[tuple(token_ids_window)] + 1\n",
    "        except KeyError:\n",
    "            cnt_target = 1\n",
    "\n",
    "        # Prevent side effect\n",
    "        token_ids_window_tmp = token_ids_window.copy()\n",
    "\n",
    "        cnt_list = []\n",
    "        for token_id in range(self.vocab_size):\n",
    "            \n",
    "            token_ids_window_tmp[-1] = token_id\n",
    "\n",
    "            try:\n",
    "                cnt = self.cnts[tuple(token_ids_window_tmp)] + 1\n",
    "            except KeyError:\n",
    "                cnt = 1\n",
    "\n",
    "            cnt_list.append(cnt)\n",
    "            \n",
    "        cnts = np.sum(cnt_list)\n",
    "\n",
    "        if cnts == 0:\n",
    "            return 1 / self.vocab_size\n",
    "\n",
    "        prob = cnt_target / cnts \n",
    "\n",
    "        return prob\n",
    "    \n",
    "\n",
    "    # No Laplace-smoothing -> trigger KeyError (later used for backoff)\n",
    "    def get_prob_raiseKeyError(self, token_ids_window):\n",
    "\n",
    "        try:\n",
    "            cnt_target = self.cnts[tuple(token_ids_window)]\n",
    "        except KeyError:\n",
    "            # unigram word not present -> return avg prob\n",
    "            if self.n == 1:\n",
    "                p = 0\n",
    "\n",
    "                for token_id in range(self.vocab_size):\n",
    "                    p += self.get_prob([token_id])\n",
    "                \n",
    "                return p / self.vocab_size \n",
    "            \n",
    "            raise KeyError\n",
    "        \n",
    "        # Prevent side effect\n",
    "        token_ids_window_tmp = token_ids_window.copy()\n",
    "\n",
    "        cnt_list = []\n",
    "        for token_id in range(self.vocab_size):\n",
    "            \n",
    "            token_ids_window_tmp[-1] = token_id\n",
    "\n",
    "            try:\n",
    "                cnt = self.cnts[tuple(token_ids_window_tmp)]\n",
    "            except KeyError:\n",
    "                cnt = 0\n",
    "\n",
    "            cnt_list.append(cnt)\n",
    "            \n",
    "        cnts = np.sum(cnt_list)\n",
    "\n",
    "        if cnts == 0:\n",
    "            return 1 / self.vocab_size\n",
    "\n",
    "        prob = cnt_target / cnts \n",
    "\n",
    "        return prob\n",
    "\n",
    "\n",
    "    def get_distri(self, token_ids_window):\n",
    "\n",
    "\n",
    "        # Prevent side effect\n",
    "        token_ids_window_tmp = token_ids_window.copy()\n",
    "\n",
    "        token_ids_window_tmp.append(None) # dummy\n",
    "\n",
    "        cnt_tokens_list = []\n",
    "        for token_id in range(self.vocab_size):\n",
    "\n",
    "            token_ids_window_tmp[-1] = token_id\n",
    "\n",
    "            try:\n",
    "                cnt = self.cnts[tuple(token_ids_window_tmp)] + 1\n",
    "            except KeyError:\n",
    "                cnt = 1\n",
    "\n",
    "            cnt_tokens_list.append(cnt)\n",
    "        \n",
    "        \n",
    "        cnt_tokens = np.array(cnt_tokens_list)\n",
    "\n",
    "        distri = cnt_tokens / np.sum(cnt_tokens)\n",
    "\n",
    "        return distri\n",
    "\n",
    "\n",
    "    def perplexity(self, corpus_val_tokenized):\n",
    "        \n",
    "        print(\"Compute perplexity\")\n",
    "\n",
    "        tokens = corpus_val_tokenized[0:self.n]\n",
    "        token_ids_window = [self.map_token_to_id[id] for id in tokens]\n",
    "\n",
    "        logit_list = []\n",
    "        for token in tqdm.tqdm(corpus_val_tokenized[self.n:], position=0, leave=True):\n",
    "\n",
    "            prob = self.get_prob(token_ids_window)\n",
    "      \n",
    "            logit = np.log(prob)\n",
    "            \n",
    "            logit_list.append(logit)\n",
    "\n",
    "            \n",
    "            token_id = self.map_token_to_id[token]\n",
    "            token_ids_window.append(token_id)\n",
    "\n",
    "            token_ids_window.pop(0)\n",
    "\n",
    "        tmp = - np.average(logit_list)\n",
    "       \n",
    "        return np.exp(tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
