{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf3bf23",
   "metadata": {},
   "source": [
    "## Util Definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ff6b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "def extract_test_and_vald_set(text, test_size = 0.1, validation_size = 0.1):\n",
    "    total_length = len(text)\n",
    "    test_length = int(total_length * test_size)\n",
    "    val_length = int(total_length * validation_size)\n",
    "\n",
    "    test_start_index = random.randint(0, total_length - test_length)\n",
    "    test_end_index = test_start_index + test_length\n",
    "    test_text = text[test_start_index:test_end_index]\n",
    "\n",
    "    total_length_without_test = total_length - test_length\n",
    "    val_start_index = random.randint(0, total_length_without_test - val_length)\n",
    "    val_end_index = val_start_index + val_length\n",
    "    validation_text = text[val_start_index:val_end_index]   \n",
    "\n",
    "    train_text = text[:val_start_index] + text[val_end_index:]  \n",
    "\n",
    "    return train_text, test_text, validation_text\n",
    "\n",
    "def encode_vocab_to_index(vocabulary):\n",
    "    stoi = {token:i for i, token in enumerate(vocabulary) }\n",
    "    def encode(token_list):\n",
    "        return [stoi[token] for token in token_list]\n",
    "    return encode\n",
    "\n",
    "def decode_index_to_vocab(vocabulary):\n",
    "    itos = {i: token for i, token in enumerate(vocabulary)}\n",
    "    def decode(index_list):\n",
    "        return [itos[i] for i in index_list]\n",
    "    return decode\n",
    "\n",
    "def save_vocabulary_or_merges(vocabulary, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for token in vocabulary:\n",
    "            file.write(f\"{token}\\n\")\n",
    "\n",
    "def save_document(text, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "def open_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a772a29e",
   "metadata": {},
   "source": [
    "### Hypterparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84c8da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ad668",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d63cb8a",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32e409a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class BPE():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def learner(corpus, merge_count=10):\n",
    "        corpus = corpus.lower()\n",
    "        words = [list(word) + ['_'] for word in corpus.split()]\n",
    "\n",
    "        merges = []\n",
    "\n",
    "        for m in range(merge_count):\n",
    "            vocab = defaultdict(int)\n",
    "            for word in words:\n",
    "                for i in range(len(word) - 1):\n",
    "                    pair = (word[i], word[i+1])\n",
    "                    vocab[pair] += 1\n",
    "\n",
    "            most_frequent = max(vocab, key=vocab.get)\n",
    "            merges.append(most_frequent)\n",
    "\n",
    "            new_token = ''.join(most_frequent)\n",
    "            new_words = []\n",
    "            for word in words:\n",
    "                new_word = []\n",
    "                i = 0\n",
    "                while i < len(word):\n",
    "                    # Merge durchfÃ¼hren\n",
    "                    if i < len(word) - 1 and (word[i], word[i+1]) == most_frequent:\n",
    "                        new_word.append(new_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_word.append(word[i])\n",
    "                        i += 1\n",
    "                new_words.append(new_word)\n",
    "            words = new_words  # Corpus aktualisieren\n",
    "\n",
    "            print(f\"Iteration {m+1}: merged {most_frequent}\")\n",
    "\n",
    "        vocabulary = set()\n",
    "        for word in words:\n",
    "            for token in word:\n",
    "                vocabulary.add(token)\n",
    "        #vocabulary = sorted(vocabulary)\n",
    "\n",
    "\n",
    "        return merges, vocabulary\n",
    "\n",
    "    def segmenter(corpus, merges):\n",
    "        words = [list(word) + ['_'] for word in corpus.lower().split()]\n",
    "        for merge in merges:\n",
    "            new_token = ''.join(merge)\n",
    "            new_words = []\n",
    "            for word in words:\n",
    "                new_word = []\n",
    "                i = 0\n",
    "                while i < len(word):\n",
    "                    if i < len(word) - 1 and (word[i], word[i+1]) == merge:\n",
    "                        new_word.append(new_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_word.append(word[i])\n",
    "                        i += 1\n",
    "                new_words.append(new_word)\n",
    "            words = new_words\n",
    "            tokenised_corpus = [''.join(word).strip('_') for word in words]\n",
    "            flat_tokens = [token for word in words for token in word if token != '_']\n",
    "        return flat_tokens\n",
    "\n",
    "    #text_path = 'data/shakespeare.txt'\n",
    "    #corpus = open_text_file(text_path)\n",
    "\n",
    "    #train_corpus, test_corpus = extract_test_set(corpus, test_size=0.1)\n",
    "    #save_document(train_corpus, 'data/train_corpus.txt')\n",
    "    #save_document(test_corpus, 'data/test_corpus.txt')\n",
    "\n",
    "    #train_corpus = open_text_file('data/train_corpus.txt')\n",
    "\n",
    "    #new_corpus, merges, tokens = learner(train_corpus, merge_count=200)\n",
    "    #save_vocabulary(tokens, 'data/vocabulary.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c77cc",
   "metadata": {},
   "source": [
    "### Splitting Corpus in train, test, and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5040f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text, test_text, validation_text = extract_test_and_vald_set(open_text_file('data/shakespeare.txt'))\n",
    "train_corpus = open_text_file('data/Shakespeare_clean_train.txt')\n",
    "test_corpus = open_text_file('data/Shakespeare_clean_test.txt')\n",
    "validation_corpus = open_text_file('data/Shakespeare_clean_valid.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae483ccd",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be5ca3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Text Sample: The Tragedy of Antony and Cleopatra Dramatis Personae MARK ANTONY OCTAVIUS CAESAR M. AEMILIUS LEPIDU\n",
      "Init Vocabulary:  !&',-.12689:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 67\n",
      "Iteration 1: merged ('e', '_')\n",
      "Iteration 2: merged ('t', 'h')\n",
      "Iteration 3: merged (',', '_')\n",
      "Iteration 4: merged ('t', '_')\n",
      "Iteration 5: merged ('s', '_')\n",
      "Iteration 6: merged ('d', '_')\n",
      "Iteration 7: merged ('a', 'n')\n",
      "Iteration 8: merged ('e', 'r')\n",
      "Iteration 9: merged ('o', 'u')\n",
      "Iteration 10: merged ('i', 'n')\n",
      "Iteration 11: merged ('o', '_')\n",
      "Iteration 12: merged ('y', '_')\n",
      "Iteration 13: merged ('e', 'n')\n",
      "Iteration 14: merged ('.', '_')\n",
      "Iteration 15: merged ('o', 'r')\n",
      "Tokenised Training Corpus Sample: ['th', 'e_', 't', 'r', 'a', 'g', 'e', 'd', 'y_', 'o', 'f', 'an', 't', 'o', 'n', 'y_', 'an', 'd_', 'c', 'l', 'e', 'o', 'p', 'a', 't', 'r', 'a', 'd', 'r', 'a', 'm', 'a', 't', 'i', 's_', 'p', 'er', 's', 'o', 'n', 'a', 'e_', 'm', 'a', 'r', 'k', 'an', 't', 'o', 'n', 'y_', 'o', 'c', 't', 'a', 'v', 'i', 'u', 's_', 'c', 'a', 'e', 's', 'a', 'r', 'm', '._', 'a', 'e', 'm', 'i', 'l', 'i', 'u', 's_', 'l', 'e', 'p', 'i', 'd', 'u', 's_', 't', 'r', 'i', 'u', 'm', 'v', 'i', 'r', 's', '._', 's', 'e', 'x', 't', 'u', 's_', 'p', 'o']\n",
      "Encoded Training Corpus Sample: [0, 10, 9, 12, 5, 32, 34, 13, 48, 53, 38, 1, 9, 53, 3, 48, 1, 40, 36, 41, 34, 53, 30, 5, 9, 12, 5, 13, 12, 5, 22, 5, 9, 26, 23, 30, 51, 7, 53, 3, 5, 10, 22, 5, 12, 49, 1, 9, 53, 3, 48, 53, 36, 9, 5, 16, 26, 6, 23, 36, 5, 34, 7, 5, 12, 22, 43, 5, 34, 22, 26, 41, 26, 6, 23, 41, 34, 30, 26, 13, 6, 23, 9, 12, 26, 6, 22, 16, 26, 12, 7, 43, 7, 34, 54, 9, 6, 23, 30, 53]\n",
      "decoded Training Ids: ['th', 'e_', 't', 'r', 'a', 'g', 'e', 'd', 'y_', 'o', 'f', 'an', 't', 'o', 'n', 'y_', 'an', 'd_', 'c', 'l', 'e', 'o', 'p', 'a', 't', 'r', 'a', 'd', 'r', 'a', 'm', 'a', 't', 'i', 's_', 'p', 'er', 's', 'o', 'n', 'a', 'e_', 'm', 'a', 'r', 'k', 'an', 't', 'o', 'n', 'y_', 'o', 'c', 't', 'a', 'v', 'i', 'u', 's_', 'c', 'a', 'e', 's', 'a', 'r', 'm', '._', 'a', 'e', 'm', 'i', 'l', 'i', 'u', 's_', 'l', 'e', 'p', 'i', 'd', 'u', 's_', 't', 'r', 'i', 'u', 'm', 'v', 'i', 'r', 's', '._', 's', 'e', 'x', 't', 'u', 's_', 'p', 'o']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Take a look at the text\n",
    "print(\"Training Text Sample:\", train_corpus[:100])\n",
    "# Take a look at the current characters that occur\n",
    "chars = sorted(list(set(train_corpus)))\n",
    "print(\"Init Vocabulary:\", ''.join(chars))\n",
    "print(\"Vocabulary size:\", len(chars))\n",
    "\n",
    "# Train the BPE model\n",
    "merges, vocabulary = BPE.learner(train_corpus, merge_count=15)\n",
    "\n",
    "# Save the vocabulary and merges\n",
    "save_vocabulary_or_merges(vocabulary, 'data/vocabulary.txt')\n",
    "save_vocabulary_or_merges(merges, 'data/merges.txt')\n",
    "\n",
    "# Apply the Segmenter to the training corpus\n",
    "tokenised_train_corpus = BPE.segmenter(train_corpus, merges)\n",
    "tokenised_validation_corpus = BPE.segmenter(validation_corpus, merges)\n",
    "tokenised_test_corpus = BPE.segmenter(test_corpus, merges)\n",
    "\n",
    "print(\"Tokenised Training Corpus Sample:\", tokenised_train_corpus[:100])  # Print first 100 tokens\n",
    "\n",
    "# Encode and decode the vocabulary to a list of indices\n",
    "encode_vocab = encode_vocab_to_index(vocabulary)\n",
    "decode_vocab = decode_index_to_vocab(vocabulary)\n",
    "\n",
    "# Encode and decode the training corpus\n",
    "train_ids = encode_vocab(tokenised_train_corpus)\n",
    "test_ids = encode_vocab(tokenised_test_corpus)\n",
    "validation_ids = encode_vocab(tokenised_validation_corpus)\n",
    "\n",
    "print(\"Encoded Training Corpus Sample:\", train_ids[:100]) \n",
    "text =  decode_vocab(train_ids)\n",
    "print(\"decoded Training Ids:\", text[:100])  # Print first 100 decoded tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ca81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([633285]) torch.int64\n",
      "Training Corpus Encoded Sample: tensor([ 0, 10,  9, 12,  5, 32, 34, 13, 48, 53, 38,  1,  9, 53,  3, 48,  1, 40,\n",
      "        36, 41, 34, 53, 30,  5,  9, 12,  5, 13, 12,  5, 22,  5,  9, 26, 23, 30,\n",
      "        51,  7, 53,  3,  5, 10, 22,  5, 12, 49,  1,  9, 53,  3, 48, 53, 36,  9,\n",
      "         5, 16, 26,  6, 23, 36,  5, 34,  7,  5, 12, 22, 43,  5, 34, 22, 26, 41,\n",
      "        26,  6, 23, 41, 34, 30, 26, 13,  6, 23,  9, 12, 26,  6, 22, 16, 26, 12,\n",
      "         7, 43,  7, 34, 54,  9,  6, 23, 30, 53])\n"
     ]
    }
   ],
   "source": [
    "# convert the training corpus to a tensor\n",
    "train_data = torch.tensor(train_ids, dtype=torch.long)\n",
    "test_data = torch.tensor(test_ids, dtype=torch.long)\n",
    "validation_data = torch.tensor(validation_ids, dtype=torch.long)\n",
    "\n",
    "print(train_data.shape, train_data.dtype)\n",
    "print(\"Training Corpus Encoded Sample:\", train_data[:100])  # Print first 100 encoded tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3741eeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: tensor([0]), Target: 10\n",
      "Context: tensor([ 0, 10]), Target: 9\n",
      "Context: tensor([ 0, 10,  9]), Target: 12\n",
      "Context: tensor([ 0, 10,  9, 12]), Target: 5\n",
      "Context: tensor([ 0, 10,  9, 12,  5]), Target: 32\n",
      "Context: tensor([ 0, 10,  9, 12,  5, 32]), Target: 34\n",
      "Context: tensor([ 0, 10,  9, 12,  5, 32, 34]), Target: 13\n",
      "Context: tensor([ 0, 10,  9, 12,  5, 32, 34, 13]), Target: 48\n"
     ]
    }
   ],
   "source": [
    "input = train_data[:block_size]\n",
    "expected_output = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = input[:t+1]\n",
    "    target = expected_output[t] \n",
    "    print(f\"Context: {context}, Target: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a4de19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 10,  9,  ...,  6,  3, 18])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efca9a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = train_data\n",
    "    elif split == 'val':\n",
    "        data = validation_data\n",
    "    #elif split == 'test':\n",
    "    #    data = test_ids\n",
    "    else:\n",
    "        raise ValueError(\"Invalid split. Choose from 'train', 'val', or 'test'.\")\n",
    "\n",
    "    intx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    input = torch.stack([data[i:i+block_size] for i in intx])\n",
    "    expected_output = torch.stack([data[i+1:i+block_size+1] for i in intx])\n",
    "\n",
    "    return input, expected_output\n",
    "\n",
    "inputMatrix, outputMatrix = get_batch('train')\n",
    "print(\"Input Matrix Shape:\", inputMatrix.shape)  \n",
    "print(\"Input Matrix:\", inputMatrix)\n",
    "print(\"Output Matrix Shape:\", outputMatrix.shape)\n",
    "print(\"Output Matrix:\", outputMatrix) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f6edef",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a357e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
