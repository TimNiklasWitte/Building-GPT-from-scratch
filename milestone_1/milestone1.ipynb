{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a2cd8e6",
   "metadata": {},
   "source": [
    "# Programming GPT from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c71d548",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623f7f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09182166",
   "metadata": {},
   "source": [
    "### Milestone 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "677dc8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', '', ''), ('A.', '', ''), ('', '-print', ''), ('', '', ''), ('', '', '.40'), ('', '', '')]\n"
     ]
    }
   ],
   "source": [
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "\n",
    "pattern = r'''(?x)            # verbose mode\n",
    "    ([A-Z]\\.)+              # abbreviations, e.g. U.S.A.\n",
    "  | \\w+(-\\w+)*              # words with optional internal hyphens\n",
    "  | \\$?\\d+(\\.\\d+)?%?        # currency and percentages\n",
    "  | \\.\\.\\.                    # ellipsis\n",
    "  | [][.,;\"'?():-_]           # these are separate tokens; includes ], [\n",
    "'''\n",
    "\n",
    "tokens = nltk.regexp_tokenize(text, pattern)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7ffd3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n"
     ]
    }
   ],
   "source": [
    "# class example\n",
    "# tr -sc 'A-Za-z' '\\n' <shakes.text\n",
    "\n",
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "\n",
    "pattern = r'''(?x)            # verbose mode\n",
    "    (?:[A-Z]\\.)+              # abbreviations, e.g. U.S.A.\n",
    "  | \\w+(?:-\\w+)*              # words with optional internal hyphens\n",
    "  | \\$?\\d+(?:\\.\\d+)?%?        # currency and percentages\n",
    "  | \\.\\.\\.                    # ellipsis\n",
    "  | [][.,;\"'?():-_]           # these are separate tokens; includes ], [\n",
    "'''\n",
    "\n",
    "tokens = nltk.regexp_tokenize(text, pattern)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38763258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n"
     ]
    }
   ],
   "source": [
    "# Alternative using RegexpTokenizer from nltk\n",
    "\n",
    "tokenizer = RegexpTokenizer(\n",
    "    r'''(?x)               # verbose\n",
    "        (?:[A-Z]\\.)+       # U.S.A.\n",
    "      | \\w+(?:-\\w+)*       # hyphenated words\n",
    "      | \\$?\\d+(?:\\.\\d+)?%? # numbers, currency, percents\n",
    "      | \\.\\.\\.             # ellipsis\n",
    "      | [][.,;\"'?():-_]    # punctuation\n",
    "    '''\n",
    ")\n",
    "print(tokenizer.tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b87998",
   "metadata": {},
   "source": [
    "### BPE token learner algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aa17afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe(c, k): # function byte-paire encoding(string c, number of merges k) returns vocab V.\n",
    "    \"\"\"\n",
    "    BPE token learner algorithm\n",
    "    :param c: input string\n",
    "    :param k: number of merges\n",
    "    :return: vocabulary V\n",
    "    \"\"\"\n",
    "    # Initialize vocabulary with characters in c\n",
    "    vocab = {char: 1 for char in c}\n",
    "    \n",
    "    # Perform k merges\n",
    "    for _ in range(k):\n",
    "        # Find the most frequent pair of characters\n",
    "        pairs = {}\n",
    "        for i in range(len(c) - 1):\n",
    "            pair = c[i:i + 2]\n",
    "            if pair in pairs:\n",
    "                pairs[pair] += 1\n",
    "            else:\n",
    "                pairs[pair] = 1\n",
    "        \n",
    "        # Find the most frequent pair\n",
    "        most_frequent_pair = max(pairs, key=pairs.get)\n",
    "        \n",
    "        # Merge the most frequent pair in the vocabulary\n",
    "        new_char = ''.join(most_frequent_pair)\n",
    "        vocab[new_char] = pairs[most_frequent_pair]\n",
    "        \n",
    "        # Update the string c by replacing the most frequent pair with the new character\n",
    "        c = c.replace(most_frequent_pair, new_char)\n",
    "    \n",
    "    return vocab    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12c1ee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T': 1, 'h': 1, 'a': 1, 't': 1, ' ': 1, 'U': 1, '.': 1, 'S': 1, 'A': 1, 'p': 1, 'o': 1, 's': 1, 'e': 1, 'r': 1, '-': 1, 'i': 1, 'n': 1, 'c': 1, '$': 1, '1': 1, '2': 1, '4': 1, '0': 1, 't ': 2}\n"
     ]
    }
   ],
   "source": [
    "tokens = bpe(text, 200)  # Example usage of BPE with 10 merges\n",
    "print(tokens)  # Output the vocabulary after BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():    \n",
    "    #\n",
    "    # Load data\n",
    "    #\n",
    "\n",
    "    file_path = \"./../data/shakespeare.txt\"\n",
    "    with open(file_path) as file:\n",
    "        corpus = file.read()\n",
    "\n",
    "    #\n",
    "    # Split into train and test\n",
    "    #\n",
    "    corpus = np.array(corpus.split(\"\\n\"))\n",
    "    num_lines = len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a18906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Shakespeare corpus into train/valid/test splits based on line sampling\n",
    "\n",
    "import random\n",
    "\n",
    "def split_shakespeare(\n",
    "    input_path: str,\n",
    "    test_frac: float = 0.01,\n",
    "    seed: int = 42,\n",
    "    out_prefix: str = \"shakespeare\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads the input text file, shuffles its lines, and splits them into:\n",
    "      - test set: 1% of lines\n",
    "      - training set: remaining 99% of lines\n",
    "\n",
    "    Saves the splits as:\n",
    "      {out_prefix}.test.txt\n",
    "      {out_prefix}.train.txt\n",
    "    \"\"\"\n",
    "    # Read all lines\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Shuffle for randomness (reproducible via seed)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(lines)\n",
    "\n",
    "    n_total = len(lines)\n",
    "    n_test = int(n_total * test_frac)\n",
    "\n",
    "    test_lines = lines[:n_test]\n",
    "    train_lines = lines[n_test:]\n",
    "\n",
    "    # Write out the splits\n",
    "    splits = {\n",
    "        f\"{out_prefix}.test.txt\": test_lines,\n",
    "        f\"{out_prefix}.valid.txt\": valid_lines,\n",
    "        f\"{out_prefix}.train.txt\": train_lines,\n",
    "    }\n",
    "    for filename, subset in splits.items():\n",
    "        with open(filename, 'w', encoding='utf-8') as out_f:\n",
    "            out_f.writelines(subset)\n",
    "        print(f\"Wrote {len(subset)} lines to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    split_shakespeare(\"shakespeare.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f7371",
   "metadata": {},
   "source": [
    "### implimentation BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3541b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Byte-Pair Encoding with Detailed Debugging for Jupyter\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "class BytePairEncodingDebug:\n",
    "    def __init__(self, num_merges):\n",
    "        \"\"\"\n",
    "        num_merges: number of BPE merge operations (k)\n",
    "        \"\"\"\n",
    "        self.num_merges = num_merges\n",
    "        # Initialize vocab to all printable ASCII (lowercase only) + newline\n",
    "        self.vocab = [chr(i) for i in range(128) if chr(i).isprintable() and not chr(i).isupper()]\n",
    "        self.vocab.append(\"\\n\")\n",
    "\n",
    "    def normalize(self, word):\n",
    "        # Lowercase normalization\n",
    "        return word.lower()\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Train BPE on `corpus` (a single string).\n",
    "        Prints detailed info at each merge step.\n",
    "        Returns the final vocabulary list.\n",
    "        \"\"\"\n",
    "        # 1) Split into \"words\" (space-separated) + count frequencies\n",
    "        map_word_cnt = defaultdict(int)\n",
    "        map_word_tokens = {}\n",
    "        for raw_line in corpus.split(\"\\n\"):\n",
    "            for w in raw_line.split(\" \"):\n",
    "                word = self.normalize(w) + \" \"  # add end-of-word marker\n",
    "                map_word_cnt[word] += 1\n",
    "                if word not in map_word_tokens:\n",
    "                    map_word_tokens[word] = list(word)\n",
    "\n",
    "        # 2) Perform merges\n",
    "        for merge_i in range(1, self.num_merges + 1):\n",
    "            # 2.1) Count all adjacent pairs\n",
    "            pair_counts = defaultdict(int)\n",
    "            for word, tokens in map_word_tokens.items():\n",
    "                freq = map_word_cnt[word]\n",
    "                for j in range(len(tokens) - 1):\n",
    "                    pair = (tokens[j], tokens[j+1])\n",
    "                    pair_counts[pair] += freq\n",
    "\n",
    "            # Debug print: top 5 most common pairs before merge\n",
    "            top5 = sorted(pair_counts.items(), key=lambda x: -x[1])[:5]\n",
    "            print(f\"[Merge {merge_i}] Top-5 candidate pairs:\", top5)\n",
    "\n",
    "            # 2.2) Select the most frequent\n",
    "            (tL, tR), count = max(pair_counts.items(), key=lambda x: x[1])\n",
    "            new_token = tL + tR\n",
    "\n",
    "            # Debug print: which pair is chosen\n",
    "            print(f\"[Merge {merge_i}] Chosen pair: ({tL!r}, {tR!r}) -> New token: {new_token!r} (count={count})\")\n",
    "\n",
    "            # 2.3) Add to vocab\n",
    "            self.vocab.append(new_token)\n",
    "\n",
    "            # 2.4) Replace in all words\n",
    "            for word, tokens in map_word_tokens.items():\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(tokens):\n",
    "                    if i+1 < len(tokens) and tokens[i] == tL and tokens[i+1] == tR:\n",
    "                        new_tokens.append(new_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_tokens.append(tokens[i])\n",
    "                        i += 1\n",
    "                map_word_tokens[word] = new_tokens\n",
    "\n",
    "            print(f\"[Merge {merge_i}] Vocab size now: {len(self.vocab)}\\n\" + \"-\"*50)\n",
    "\n",
    "        # 3) Sort vocab longest-first for greedy tokenization\n",
    "        self.vocab.sort(key=len, reverse=True)\n",
    "        return self.vocab\n",
    "\n",
    "    def segment(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize `text` using the learned BPE vocab.\n",
    "        \"\"\"\n",
    "        tokens_out = []\n",
    "        for line in text.split(\"\\n\"):\n",
    "            for w in line.split(\" \"):\n",
    "                word = self.normalize(w) + \" \"\n",
    "                tokens_out.extend(self._tokenize_word(word))\n",
    "        return tokens_out[:-1]  # drop final dummy space\n",
    "\n",
    "    def _tokenize_word(self, word):\n",
    "        \"\"\"\n",
    "        Greedy longest-match tokenization of a single word+space.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        out = []\n",
    "        while i < len(word):\n",
    "            for t in self.vocab:\n",
    "                if word.startswith(t, i):\n",
    "                    out.append(t)\n",
    "                    i += len(t)\n",
    "                    break\n",
    "        return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e4ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Merge 1] Top-5 candidate pairs: [(('e', ' '), 129069), (('t', 'h'), 113368), (('h', 'e'), 83033), ((',', ' '), 82969), (('t', ' '), 77622)]\n",
      "[Merge 1] Chosen pair: ('e', ' ') -> New token: 'e ' (count=129069)\n",
      "[Merge 1] Vocab size now: 71\n",
      "--------------------------------------------------\n",
      "[Merge 2] Top-5 candidate pairs: [(('t', 'h'), 113368), ((',', ' '), 82969), (('t', ' '), 77622), (('.', ' '), 76602), (('s', ' '), 75157)]\n",
      "[Merge 2] Chosen pair: ('t', 'h') -> New token: 'th' (count=113368)\n",
      "[Merge 2] Vocab size now: 72\n",
      "--------------------------------------------------\n",
      "[Merge 3] Top-5 candidate pairs: [((',', ' '), 82969), (('t', ' '), 77622), (('.', ' '), 76602), (('s', ' '), 75157), (('d', ' '), 67811)]\n",
      "[Merge 3] Chosen pair: (',', ' ') -> New token: ', ' (count=82969)\n",
      "[Merge 3] Vocab size now: 73\n",
      "--------------------------------------------------\n",
      "[Merge 4] Top-5 candidate pairs: [(('t', ' '), 77622), (('.', ' '), 76602), (('s', ' '), 75157), (('d', ' '), 67811), (('a', 'n'), 61157)]\n",
      "[Merge 4] Chosen pair: ('t', ' ') -> New token: 't ' (count=77622)\n",
      "[Merge 4] Vocab size now: 74\n",
      "--------------------------------------------------\n",
      "[Merge 5] Top-5 candidate pairs: [(('.', ' '), 76602), (('s', ' '), 75157), (('d', ' '), 67811), (('a', 'n'), 61157), (('e', 'r'), 58791)]\n",
      "[Merge 5] Chosen pair: ('.', ' ') -> New token: '. ' (count=76602)\n",
      "[Merge 5] Vocab size now: 75\n",
      "--------------------------------------------------\n",
      "[Merge 6] Top-5 candidate pairs: [(('s', ' '), 75157), (('d', ' '), 67811), (('a', 'n'), 61157), (('e', 'r'), 58791), (('o', 'u'), 55502)]\n",
      "[Merge 6] Chosen pair: ('s', ' ') -> New token: 's ' (count=75157)\n",
      "[Merge 6] Vocab size now: 76\n",
      "--------------------------------------------------\n",
      "[Merge 7] Top-5 candidate pairs: [(('d', ' '), 67811), (('a', 'n'), 61157), (('e', 'r'), 58791), (('o', 'u'), 55502), (('i', 'n'), 53781)]\n",
      "[Merge 7] Chosen pair: ('d', ' ') -> New token: 'd ' (count=67811)\n",
      "[Merge 7] Vocab size now: 77\n",
      "--------------------------------------------------\n",
      "[Merge 8] Top-5 candidate pairs: [(('a', 'n'), 61157), (('e', 'r'), 58791), (('o', 'u'), 55502), (('i', 'n'), 53781), (('r', ' '), 50954)]\n",
      "[Merge 8] Chosen pair: ('a', 'n') -> New token: 'an' (count=61157)\n",
      "[Merge 8] Vocab size now: 78\n",
      "--------------------------------------------------\n",
      "[Merge 9] Top-5 candidate pairs: [(('e', 'r'), 58791), (('o', 'u'), 55502), (('i', 'n'), 53781), (('r', ' '), 50954), (('y', ' '), 45673)]\n",
      "[Merge 9] Chosen pair: ('e', 'r') -> New token: 'er' (count=58791)\n",
      "[Merge 9] Vocab size now: 79\n",
      "--------------------------------------------------\n",
      "[Merge 10] Top-5 candidate pairs: [(('o', 'u'), 55502), (('i', 'n'), 53781), (('y', ' '), 45673), (('o', 'r'), 40601), (('e', 'n'), 39400)]\n",
      "[Merge 10] Chosen pair: ('o', 'u') -> New token: 'ou' (count=55502)\n",
      "[Merge 10] Vocab size now: 80\n",
      "--------------------------------------------------\n",
      "Final vocab: ['e ', 'th', ', ', 't ', '. ', 's ', 'd ', 'an', 'er', 'ou', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\n']\n",
      "\n",
      "\n",
      "\n",
      "['t', 'o', ' ', 'b', 'e ', 'o', 'r', ' ', 'n', 'o', 't ', 't', 'o', ' ', 'b']\n"
     ]
    }
   ],
   "source": [
    "# 1) Load your corpus:\n",
    "with open(\"data/shakespeare.txt\",\"r\") as f:\n",
    "    text = f.read()\n",
    "#\n",
    "# 2) Instantiate with desired merges (e.g., 10):\n",
    "bpe = BytePairEncodingDebug(num_merges=10)\n",
    "#\n",
    "# 3) Train and watch the debug prints for each merge:\n",
    "vocab = bpe.train(text)\n",
    "#\n",
    "# 4) Inspect the final vocab:\n",
    "print(\"Final vocab:\", vocab)\n",
    "#\n",
    "print('\\n\\n')\n",
    "# 5) Segment some sample sentences:\n",
    "print(bpe.segment(f\"To be or not to be\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
